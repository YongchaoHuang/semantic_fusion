{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# begin."
      ],
      "metadata": {
        "id": "05hLE0ZEtlfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# simple corpus."
      ],
      "metadata": {
        "id": "vtnef39uGyE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random, re\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------------\n",
        "# 0) Utilities / Repro\n",
        "# -------------------------------\n",
        "SEED = 111\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def pad_to_len(x, L, pad=0):\n",
        "    return x + [pad] * (L - len(x))\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Synthetic corpus generator\n",
        "# -------------------------------\n",
        "SUBJECTS = [\"Alice\", \"Bob\", \"Carol\", \"Dave\", \"Eve\"]\n",
        "OBJECTS  = [\"task\", \"paper\", \"model\", \"project\", \"meal\"]\n",
        "VERBS    = [\"finishes\", \"reviews\", \"trains\", \"starts\", \"cooks\"]\n",
        "ADJ_POS  = [\"good\", \"great\", \"excellent\", \"pleasant\", \"wonderful\"]\n",
        "ADJ_NEG  = [\"bad\", \"poor\", \"terrible\", \"unpleasant\", \"awful\"]\n",
        "INTENS   = [\"slightly\", \"moderately\", \"very\", \"extremely\"]\n",
        "PUNCT    = [\".\", \"!\", \"?\"]\n",
        "COMMAS   = [\",\"]\n",
        "CONJ     = [\"and\", \"but\"]\n",
        "\n",
        "SUBJ2PRON = {\"Alice\":\"she\",\"Bob\":\"he\",\"Carol\":\"she\",\"Dave\":\"he\",\"Eve\":\"she\"}\n",
        "DEFAULT_PRON = \"they\"\n",
        "\n",
        "def _rand_adj():\n",
        "    is_pos = random.random() < 0.5\n",
        "    return random.choice(ADJ_POS if is_pos else ADJ_NEG), is_pos\n",
        "\n",
        "def _clause_tokens(subj=None, force_pron=False):\n",
        "    if subj is None:\n",
        "        subj = random.choice(SUBJECTS)\n",
        "    head = SUBJ2PRON.get(subj, DEFAULT_PRON) if force_pron else subj\n",
        "    verb = random.choice(VERBS)\n",
        "    obj  = random.choice(OBJECTS)\n",
        "    intens = random.choices(INTENS, weights=[2,2,3,2], k=1)[0]\n",
        "    adj, _ = _rand_adj()\n",
        "    toks = [head, verb, \"the\", obj, \",\", intens, adj]\n",
        "    toks = [t if (t in SUBJECTS or t in PUNCT or t in COMMAS) else t.lower() for t in toks]\n",
        "    return toks\n",
        "\n",
        "def make_corpus(n_train=8000, n_val=1200, max_len=28,\n",
        "                adj_pos_train=None, adj_neg_train=None,\n",
        "                adj_pos_val=None,   adj_neg_val=None):\n",
        "    adj_pos_train = adj_pos_train if adj_pos_train is not None else ADJ_POS\n",
        "    adj_neg_train = adj_neg_train if adj_neg_train is not None else ADJ_NEG\n",
        "    adj_pos_val   = adj_pos_val   if adj_pos_val   is not None else ADJ_POS\n",
        "    adj_neg_val   = adj_neg_val   if adj_neg_val   is not None else ADJ_NEG\n",
        "\n",
        "    def sample_with_lists(use_pos, use_neg, two_clause_p=0.6):\n",
        "        subj = random.choice(SUBJECTS)\n",
        "        c1 = _clause_tokens(subj=subj, force_pron=False)\n",
        "        if c1[-1] in ADJ_POS + ADJ_NEG:\n",
        "            c1[-1] = random.choice(use_pos if random.random() < 0.5 else use_neg)\n",
        "        if random.random() < two_clause_p:\n",
        "            conj = random.choice(CONJ)\n",
        "            c2 = _clause_tokens(subj=subj, force_pron=True)\n",
        "            if c2[-1] in ADJ_POS + ADJ_NEG:\n",
        "                c2[-1] = random.choice(use_pos if random.random() < 0.5 else use_neg)\n",
        "            end = random.choices(PUNCT, weights=[8,3,1], k=1)[0]\n",
        "            sent = c1 + [conj] + c2 + [end]\n",
        "        else:\n",
        "            end = random.choices(PUNCT, weights=[8,3,1], k=1)[0]\n",
        "            sent = c1 + [end]\n",
        "        return [t if (t in SUBJECTS or t in PUNCT or t in COMMAS) else t.lower() for t in sent]\n",
        "\n",
        "    train = [sample_with_lists(adj_pos_train, adj_neg_train) for _ in range(n_train)]\n",
        "    val   = [sample_with_lists(adj_pos_val,   adj_neg_val)   for _ in range(n_val)]\n",
        "    train = [s[:max_len] for s in train]\n",
        "    val   = [s[:max_len] for s in val]\n",
        "    return train, val\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Vocab + tokenization\n",
        "# -------------------------------\n",
        "SPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "def build_vocab(sents: List[List[str]]):\n",
        "    vocab = {tok for s in sents for tok in s}\n",
        "    stoi = {sp:i for i,sp in enumerate(SPECIALS)}\n",
        "    for tok in sorted(vocab):\n",
        "        if tok not in stoi: stoi[tok] = len(stoi)\n",
        "    itos = {i:s for s,i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "def encode_sentence(tokens: List[str], stoi: Dict[str,int], max_len: int):\n",
        "    ids = [stoi[\"<bos>\"]] + [stoi[t] for t in tokens] + [stoi[\"<eos>\"]]\n",
        "    ids = ids[:max_len]\n",
        "    ids = pad_to_len(ids, max_len, stoi[\"<pad>\"])\n",
        "    return ids, len(ids)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Fuzzy semantic features\n",
        "# -------------------------------\n",
        "@dataclass\n",
        "class FeatureBank:\n",
        "    names: List[str]\n",
        "\n",
        "def power_membership(x: float, c: float, tau: float=0.5):\n",
        "    return float(pow(0.9, abs(x - c) / max(tau, 1e-6)))\n",
        "\n",
        "def tri_memberships(x: float, centers: List[float], tau: float=0.5):\n",
        "    return [power_membership(x, c, tau) for c in centers]\n",
        "\n",
        "NOUNS = set(OBJECTS + [\"research\", \"dataset\", \"result\", \"paper\", \"report\", \"code\", \"tool\"])\n",
        "VERB_SET = set(v.lower() for v in VERBS)\n",
        "ADJ_SET_POS = set(ADJ_POS)\n",
        "ADJ_SET_NEG = set(ADJ_NEG)\n",
        "INTENS_STRENGTH = {\"slightly\": 0.2, \"moderately\": 0.5, \"very\": 0.8, \"extremely\": 1.0}\n",
        "PRONOUNS = {\"he\",\"she\",\"they\"}\n",
        "\n",
        "def sentiment_score(token: str):\n",
        "    if token in ADJ_SET_POS: return 1.0\n",
        "    if token in ADJ_SET_NEG: return -1.0\n",
        "    return 0.0\n",
        "\n",
        "def strength_signal(token: str, punct: str):\n",
        "    base = INTENS_STRENGTH.get(token, 0.0)\n",
        "    if punct == \"!\": base = min(1.0, base + 0.2)\n",
        "    return base\n",
        "\n",
        "def build_feature_bank() -> FeatureBank:\n",
        "    names = [\n",
        "        \"is_noun\",\"is_verb\",\"is_adj\",\n",
        "        \"is_subject\",\"is_object\",\"is_head\",\n",
        "        \"is_bos\",\"is_eos\",\"is_comma\",\"is_question\",\n",
        "        \"pos_low\",\"pos_med\",\"pos_high\",\n",
        "        \"neg_low\",\"neg_med\",\"neg_high\",\n",
        "        \"str_low\",\"str_med\",\"str_high\",\n",
        "        \"coref_subject\",\n",
        "        \"is_capitalized\",\"is_pronoun\"\n",
        "    ]\n",
        "    return FeatureBank(names)\n",
        "\n",
        "def compute_semantics(tokens: List[str], max_len: int, fb: FeatureBank, stoi: Dict[str,int]) -> np.ndarray:\n",
        "    T = min(len(tokens)+2, max_len)\n",
        "    F = len(fb.names)\n",
        "    M = np.zeros((max_len, F), dtype=np.float32)\n",
        "\n",
        "    subj_index = 1 if T > 1 else 0\n",
        "    head_index = 2 if T > 2 else 1\n",
        "    obj_index  = 4 if T > 4 else min(T-2, 4)\n",
        "    end_tok = tokens[-1] if len(tokens)>0 else \".\"\n",
        "    end_punct = end_tok if end_tok in PUNCT else \".\"\n",
        "\n",
        "    aligned = [\"<bos>\"] + tokens[:max_len-2] + [\"<eos>\"]\n",
        "    for t in range(T):\n",
        "        tok = aligned[t]\n",
        "        is_bos = (t==0)\n",
        "        is_eos = (t==T-1)\n",
        "        is_comma = (tok == \",\")\n",
        "        is_question = (tok == \"?\")\n",
        "        is_cap = (len(tok)>0 and tok[0].isupper() and tok not in PUNCT and tok not in COMMAS)\n",
        "        is_pron = tok in PRONOUNS\n",
        "\n",
        "        is_noun = float(tok in NOUNS or (tok.isalpha() and tok.lower() not in VERB_SET and tok not in ADJ_SET_POS and tok not in ADJ_SET_NEG and tok not in SUBJECTS and tok not in PRONOUNS))\n",
        "        is_verb = float(tok.lower() in VERB_SET)\n",
        "        is_adj  = float(tok in ADJ_SET_POS or tok in ADJ_SET_NEG)\n",
        "\n",
        "        is_subj = float(t == subj_index)\n",
        "        is_head = float(t == head_index)\n",
        "        is_obj  = float(t == obj_index)\n",
        "\n",
        "        s = sentiment_score(tok)\n",
        "        pos_mag = max(0.0, s)\n",
        "        neg_mag = max(0.0, -s)\n",
        "        pos_low, pos_med, pos_high = tri_memberships(pos_mag, [0.2, 0.6, 1.0], tau=0.35)\n",
        "        neg_low, neg_med, neg_high = tri_memberships(neg_mag, [0.2, 0.6, 1.0], tau=0.35)\n",
        "\n",
        "        intens_tok = aligned[t-1].lower() if t-1 >= 0 else \"\"\n",
        "        strength = strength_signal(intens_tok, end_punct)\n",
        "        str_low, str_med, str_high = tri_memberships(strength, [0.2, 0.6, 1.0], tau=0.35)\n",
        "\n",
        "        coref_subject = float(tok in {\"he\",\"she\",\"they\"} and any(p in aligned[:t] for p in SUBJECTS))\n",
        "\n",
        "        feat = [\n",
        "            is_noun, is_verb, is_adj, is_subj, is_obj, is_head,\n",
        "            float(is_bos), float(is_eos), float(is_comma), float(is_question),\n",
        "            pos_low, pos_med, pos_high, neg_low, neg_med, neg_high,\n",
        "            str_low, str_med, str_high, coref_subject,\n",
        "            float(is_cap), float(is_pron),\n",
        "        ]\n",
        "        M[t,:] = np.array(feat, dtype=np.float32)\n",
        "\n",
        "    return M\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Dataset\n",
        "# -------------------------------\n",
        "class LMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sents, stoi, fb: FeatureBank, max_len=28):\n",
        "        self.sents = sents\n",
        "        self.stoi = stoi\n",
        "        self.fb = fb\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.sents)\n",
        "    def __getitem__(self, idx):\n",
        "        toks = self.sents[idx]\n",
        "        ids, _ = encode_sentence(toks, self.stoi, self.max_len)\n",
        "        x = torch.tensor(ids[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(ids[1:], dtype=torch.long)\n",
        "        pad_id = self.stoi[\"<pad>\"]\n",
        "        mask = (x != pad_id).float()\n",
        "        M = compute_semantics(toks, self.max_len, self.fb, self.stoi)\n",
        "        M = torch.from_numpy(M[:-1, :])\n",
        "        return x, y, mask, M\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Models\n",
        "# -------------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TinyTransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, nlayers=4, dim_ff=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model, max_len=512)\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.tok.weight  # weight tying\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        h = self.tok(x)\n",
        "        h = self.pos(h)\n",
        "        h = self.enc(h, src_key_padding_mask=attn_mask)\n",
        "        logits = self.lm_head(h)\n",
        "        return logits, h\n",
        "\n",
        "class SemanticFusionLM(nn.Module):\n",
        "    def __init__(self, vocab_size, feat_dim, d_model=128, nhead=4, nlayers=4, dim_ff=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.sem_proj = nn.Linear(feat_dim, d_model)\n",
        "        self.gate = nn.Linear(d_model + feat_dim, d_model)\n",
        "        self.pos = PositionalEncoding(d_model, max_len=512)\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.tok.weight  # weight tying\n",
        "        self.aux_head = nn.Sequential(\n",
        "            nn.Linear(d_model, max(64, feat_dim*2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(max(64, feat_dim*2), feat_dim),\n",
        "        )\n",
        "    def forward(self, x, sem, attn_mask=None):\n",
        "        e = self.tok(x)\n",
        "        s = self.sem_proj(sem)\n",
        "        g = torch.sigmoid(self.gate(torch.cat([e, sem], dim=-1)))\n",
        "        h0 = e + s + g * s\n",
        "        h0 = self.pos(h0)\n",
        "        h = self.enc(h0, src_key_padding_mask=attn_mask)\n",
        "        logits = self.lm_head(h)\n",
        "        sem_pred = torch.sigmoid(self.aux_head(h))\n",
        "        return logits, sem_pred, h\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Loss\n",
        "# -------------------------------\n",
        "LS_EPS = 0.02       # slightly lower label smoothing (favor PPL)\n",
        "UNI_LAM = 0.01      # slightly lower uniformizer (favor PPL)\n",
        "UNI_ON  = True      # toggle: set False to ablate uniformizer\n",
        "\n",
        "def lm_step(logits, y, mask, label_smoothing=0.0):\n",
        "    vocab = logits.size(-1)\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(-1, vocab),\n",
        "        y.reshape(-1),\n",
        "        reduction=\"none\",\n",
        "        label_smoothing=label_smoothing\n",
        "    )\n",
        "    loss = (loss * mask.reshape(-1)).sum() / (mask.sum() + 1e-8)\n",
        "    return loss\n",
        "\n",
        "def bce_step(pred, target, mask):\n",
        "    loss = F.binary_cross_entropy(pred, target, reduction=\"none\").mean(-1)\n",
        "    loss = (loss * mask).sum() / (mask.sum() + 1e-8)\n",
        "    return loss\n",
        "\n",
        "def adj_uniform_kl_loss(logits, y, mask, groups, lam=UNI_LAM):\n",
        "    if not UNI_ON or lam <= 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "    B, L, V = logits.size()\n",
        "    device = logits.device\n",
        "    y = y.view(-1)\n",
        "    mask = (mask.view(-1) > 0.5)\n",
        "\n",
        "    pos_ids = groups.pos_adj.to(device)\n",
        "    neg_ids = groups.neg_adj.to(device)\n",
        "\n",
        "    def kl_to_uniform(sel_mask, cls_ids):\n",
        "        if sel_mask.sum() == 0:\n",
        "            return None\n",
        "        rows = torch.nonzero(sel_mask, as_tuple=False).squeeze(-1)\n",
        "        g = logits.view(-1, V)[rows][:, cls_ids]  # [N, K]\n",
        "        p = torch.softmax(g, dim=-1)\n",
        "        K = g.size(1)\n",
        "        kl = (p * (torch.log(p + 1e-9) - math.log(1.0 / K))).sum(dim=-1)\n",
        "        return kl.mean()\n",
        "\n",
        "    losses = []\n",
        "    lp = kl_to_uniform(mask & torch.isin(y, pos_ids), pos_ids)\n",
        "    ln = kl_to_uniform(mask & torch.isin(y, neg_ids), neg_ids)\n",
        "    if lp is not None: losses.append(lp)\n",
        "    if ln is not None: losses.append(ln)\n",
        "    if not losses:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "    return lam * sum(losses) / len(losses)\n",
        "\n",
        "# -------------------------------\n",
        "# 7) Decoding helpers\n",
        "# -------------------------------\n",
        "def detokenize(ids, itos):\n",
        "    toks = [itos[i] for i in ids if itos[i] not in {\"<bos>\", \"<eos>\", \"<pad>\"}]\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if t in {\".\", \",\", \"!\", \"?\"} and out:\n",
        "            out[-1] = out[-1] + t\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return \" \".join(out)\n",
        "\n",
        "def top_p_top_k_sample(logits, temperature=1.0, top_p=0.9, top_k: int = 0):\n",
        "    \"\"\"Softmax -> optional top-k clamp -> nucleus -> sample.\"\"\"\n",
        "    if temperature <= 0:\n",
        "        return int(torch.argmax(logits).item())\n",
        "    probs = torch.softmax(logits / max(temperature, 1e-8), dim=-1)\n",
        "\n",
        "    # top-k clamp\n",
        "    if top_k and top_k > 0 and top_k < probs.numel():\n",
        "        topk_vals, topk_idx = torch.topk(probs, k=top_k, dim=-1)\n",
        "        mask = torch.full_like(probs, 0.0)\n",
        "        mask[topk_idx] = topk_vals\n",
        "        probs = mask\n",
        "        s = probs.sum()\n",
        "        if s > 0:\n",
        "            probs = probs / s\n",
        "\n",
        "    # nucleus on current probs\n",
        "    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "    cdf = torch.cumsum(sorted_probs, dim=-1)\n",
        "    keep = cdf <= top_p\n",
        "    if sorted_probs.numel() > 0:\n",
        "        keep[0] = True\n",
        "    filtered = sorted_probs[keep]\n",
        "    filtered_idx = sorted_idx[keep]\n",
        "    s = filtered.sum()\n",
        "    if s > 0:\n",
        "        filtered = filtered / s\n",
        "    choice = torch.multinomial(filtered, 1).item()\n",
        "    return int(filtered_idx[choice].item())\n",
        "\n",
        "def sample_group_mixture(next_logits: torch.Tensor,\n",
        "                         group_ids: List[int],\n",
        "                         temperature: float = 1.3,\n",
        "                         top_p: float = 0.95,\n",
        "                         alpha_uniform: float = 0.85):\n",
        "    \"\"\"\n",
        "    Sample inside group_ids from q = (1-alpha)*softmax(l/T) + alpha*Uniform(group),\n",
        "    and apply nucleus on q (NOT on p) to avoid clipping low-prob holdouts.\n",
        "    \"\"\"\n",
        "    if not group_ids:\n",
        "        return int(torch.argmax(next_logits).item())\n",
        "    idx = torch.tensor(group_ids, device=next_logits.device, dtype=torch.long)\n",
        "    g = next_logits[idx] / max(temperature, 1e-8)\n",
        "    p = torch.softmax(g, dim=-1)\n",
        "    u = torch.full_like(p, 1.0 / p.numel())\n",
        "    q = (1 - alpha_uniform) * p + alpha_uniform * u  # mix first\n",
        "\n",
        "    sp, si = torch.sort(q, descending=True)          # nucleus on q\n",
        "    cdf = torch.cumsum(sp, dim=-1)\n",
        "    keep = cdf <= top_p\n",
        "    keep[0] = True\n",
        "    sp = sp[keep]; si = si[keep]\n",
        "    sp = sp / sp.sum()\n",
        "    choice = torch.multinomial(sp, 1).item()\n",
        "    return int(idx[si[choice]].item())\n",
        "\n",
        "def simple_repetition_penalty_k(logits: torch.Tensor, prev_ids: List[int],\n",
        "                                k: int = 3, penalty: float = 2.5, itos=None):\n",
        "    \"\"\"Apply a repetition penalty to the last-k generated IDs.\"\"\"\n",
        "    l = logits.clone()\n",
        "    if not prev_ids or penalty <= 1.0:\n",
        "        return l\n",
        "    last = prev_ids[-k:] if len(prev_ids) >= k else prev_ids\n",
        "    for tid in last:\n",
        "        l[tid] -= math.log(max(penalty, 1.0001))\n",
        "    if itos is not None and prev_ids:\n",
        "        last_tok = itos[prev_ids[-1]]\n",
        "        if last_tok in {\".\", \"!\", \"?\", \",\"}:\n",
        "            l[prev_ids[-1]] -= math.log(1.8)\n",
        "    return l\n",
        "\n",
        "def build_semantics_for_prefix(prefix_ids, itos, fb, max_len):\n",
        "    L = min(len(prefix_ids), max_len)\n",
        "    toks = [itos[i] for i in prefix_ids if itos[i] not in {\"<bos>\", \"<pad>\"}]\n",
        "    M_np = compute_semantics(toks, max_len, fb, None)\n",
        "    M = torch.from_numpy(M_np[:L, :]).float()\n",
        "    if L == 0:\n",
        "        M = torch.zeros(1, len(fb.names), dtype=torch.float32)\n",
        "    return M\n",
        "\n",
        "def apply_control(M, fb, control, mode=\"last\"):\n",
        "    if not control: return M\n",
        "    name_to_idx = {n:i for i,n in enumerate(fb.names)}\n",
        "    M = M.clone()\n",
        "    idxs = [M.size(0)-1] if mode == \"last\" else list(range(M.size(0)))\n",
        "    for k,v in control.items():\n",
        "        if k in name_to_idx:\n",
        "            j = name_to_idx[k]\n",
        "            for i in idxs:\n",
        "                M[i, j] = float(v)\n",
        "    return M\n",
        "\n",
        "# -------------------------------\n",
        "# 7b) Vocab groups + steering (for 1-clause generator)\n",
        "# -------------------------------\n",
        "@dataclass\n",
        "class VocabGroups:\n",
        "    subjects: torch.Tensor\n",
        "    verbs:    torch.Tensor\n",
        "    the_id:   int\n",
        "    objects:  torch.Tensor\n",
        "    comma_id: int\n",
        "    intens:   torch.Tensor\n",
        "    pos_adj:  torch.Tensor\n",
        "    neg_adj:  torch.Tensor\n",
        "    exclam:   int\n",
        "    qmark:    int\n",
        "    period:   int\n",
        "\n",
        "def build_vocab_groups(stoi: Dict[str,int]) -> VocabGroups:\n",
        "    def ids(xs): return torch.tensor([stoi[x] for x in xs if x in stoi], dtype=torch.long, device=DEVICE)\n",
        "    subjects = ids(SUBJECTS)\n",
        "    verbs    = ids(VERBS)\n",
        "    the_id   = stoi.get(\"the\", -1)\n",
        "    objects  = ids(OBJECTS)\n",
        "    comma_id = stoi.get(\",\", -1)\n",
        "    intens   = ids(INTENS)\n",
        "    pos_adj  = ids(ADJ_POS)\n",
        "    neg_adj  = ids(ADJ_NEG)\n",
        "    exclam   = stoi.get(\"!\", -1)\n",
        "    qmark    = stoi.get(\"?\", -1)\n",
        "    period   = stoi.get(\".\", -1)\n",
        "    return VocabGroups(subjects, verbs, the_id, objects, comma_id, intens, pos_adj, neg_adj, exclam, qmark, period)\n",
        "\n",
        "def apply_logit_steer(logits: torch.Tensor, control: Dict[str,float], groups: VocabGroups, state: int):\n",
        "    \"\"\"State-aware steering: INTENS=6, ADJ=7, PUNCT=8\"\"\"\n",
        "    l = logits.clone()\n",
        "    pos = control.get(\"pos_high\", 0.0) - control.get(\"neg_high\", 0.0)\n",
        "    neg = control.get(\"neg_high\", 0.0) - control.get(\"pos_high\", 0.0)\n",
        "    strength = max(control.get(\"str_high\", 0.0), control.get(\"str_med\", 0.0))\n",
        "    is_question = control.get(\"is_question\", 0.0)\n",
        "\n",
        "    if state == 6 and groups.intens.numel() > 0 and strength > 0:\n",
        "        l[groups.intens] += 1.2 * (0.5 + strength)\n",
        "\n",
        "    if state == 7:\n",
        "        if groups.pos_adj.numel() > 0 and pos > 0:\n",
        "            l[groups.pos_adj] += 6.0 * pos\n",
        "            l[groups.neg_adj] -= 3.0 * pos\n",
        "        if groups.neg_adj.numel() > 0 and neg > 0:\n",
        "            l[groups.neg_adj] += 6.0 * neg\n",
        "            l[groups.pos_adj] -= 3.0 * neg\n",
        "\n",
        "    if state == 8:\n",
        "        if groups.exclam >= 0:\n",
        "            l[groups.exclam] += 2.8 * max(0.0, strength) * max(0.0, pos)\n",
        "        if groups.qmark >= 0:\n",
        "            l[groups.qmark] += 2.8 * is_question\n",
        "\n",
        "    return l\n",
        "\n",
        "# -------------------------------\n",
        "# 7c) Finite-State Grammar (1-clause generation)\n",
        "# -------------------------------\n",
        "@dataclass\n",
        "class Grammar:\n",
        "    start_state: int = 1\n",
        "    end_state:   int = 9\n",
        "# states: 1 SUBJ -> 2 VERB -> 3 'the' -> 4 OBJ -> 5 ',' -> 6 INTENS -> 7 ADJ -> 8 PUNCT -> 9 END\n",
        "\n",
        "def allowed_ids_for_state(state: int, groups: VocabGroups) -> List[int]:\n",
        "    if state == 1:   return groups.subjects.tolist()\n",
        "    if state == 2:   return groups.verbs.tolist()\n",
        "    if state == 3:   return [groups.the_id] if groups.the_id >= 0 else []\n",
        "    if state == 4:   return groups.objects.tolist()\n",
        "    if state == 5:   return [groups.comma_id] if groups.comma_id >= 0 else []\n",
        "    if state == 6:   return groups.intens.tolist()\n",
        "    if state == 7:   return (groups.pos_adj.tolist() + groups.neg_adj.tolist())\n",
        "    if state == 8:   return [i for i in [groups.period, groups.exclam, groups.qmark] if i >= 0]\n",
        "    return []\n",
        "\n",
        "def next_state(state: int) -> int:\n",
        "    return min(state + 1, 9)\n",
        "\n",
        "# -------------------------------\n",
        "# 7d) Prompt support (helpers)\n",
        "# -------------------------------\n",
        "PROMPT_SPLIT_RE = re.compile(r\"[A-Za-z]+|[.,!?]\")\n",
        "\n",
        "def tokenize_prompt(text: str) -> List[str]:\n",
        "    \"\"\"Split prompt into tokens consistent with this toy vocab.\"\"\"\n",
        "    toks = PROMPT_SPLIT_RE.findall(text)\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if t in SUBJECTS or t in PUNCT or t in COMMAS:\n",
        "            out.append(t)                 # keep case for names; keep punctuation/comma\n",
        "        else:\n",
        "            out.append(t.lower())         # everything else lower-cased\n",
        "    return out\n",
        "\n",
        "def infer_state_from_prefix(prefix_toks: List[str], stoi: Dict[str,int], itos: Dict[int,str], groups: \"VocabGroups\") -> int:\n",
        "    \"\"\"Validate prefix against the FSG and return the next decoding state.\"\"\"\n",
        "    grammar = Grammar()\n",
        "    state = grammar.start_state\n",
        "    for t in prefix_toks:\n",
        "        if state == grammar.end_state:\n",
        "            break\n",
        "        if t not in stoi:\n",
        "            raise ValueError(f\"Prompt token '{t}' not in vocabulary.\")\n",
        "        allow = set(allowed_ids_for_state(state, groups))\n",
        "        tid = stoi[t]\n",
        "        if tid in allow:\n",
        "            state = next_state(state)\n",
        "        else:\n",
        "            expected = [itos[i] for i in allow]\n",
        "            raise ValueError(f\"Prompt token '{t}' invalid at grammar state {state}. \"\n",
        "                             f\"Expected one of {expected}.\")\n",
        "    return state\n",
        "\n",
        "# -------------------------------\n",
        "# 7e) Baseline *fair* generator (grammar + last-3 repetition penalty) with prompt\n",
        "# -------------------------------\n",
        "_VOCAB_GROUPS: VocabGroups = None  # set in run_experiment()\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_baseline_fair(model, stoi, itos, max_len=28, temperature=0.8, top_p=0.9, top_k=20,\n",
        "                           prompt: Optional[str] = None):\n",
        "    model.eval()\n",
        "    groups = _VOCAB_GROUPS\n",
        "    grammar = Grammar()\n",
        "    bos, eos, pad = stoi[\"<bos>\"], stoi[\"<eos>\"], stoi[\"<pad>\"]\n",
        "    ids = [bos]\n",
        "    state = grammar.start_state\n",
        "\n",
        "    # --- Prompt prefix handling ---\n",
        "    if prompt:\n",
        "        prefix_toks = tokenize_prompt(prompt)\n",
        "        state = infer_state_from_prefix(prefix_toks, stoi, itos, groups)\n",
        "        ids += [stoi[t] for t in prefix_toks]\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        x = torch.tensor(ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "        attn_mask = (x == pad)\n",
        "        logits, _ = model(x, attn_mask=attn_mask)\n",
        "        next_logits = logits[0, -1, :]\n",
        "\n",
        "        # grammar mask\n",
        "        allow = allowed_ids_for_state(state, groups)\n",
        "        mask = torch.full_like(next_logits, float(\"-inf\"))\n",
        "        if allow:\n",
        "            mask[torch.tensor(allow, device=next_logits.device)] = 0.0\n",
        "        next_logits = next_logits + mask\n",
        "\n",
        "        # last-3 repetition guard (stronger)\n",
        "        next_logits = simple_repetition_penalty_k(next_logits, ids, k=3, penalty=2.5, itos=itos)\n",
        "        nid = top_p_top_k_sample(next_logits, temperature, top_p, top_k=top_k)\n",
        "        ids.append(nid)\n",
        "\n",
        "        state = next_state(state)\n",
        "        if state == grammar.end_state:\n",
        "            break\n",
        "    ids.append(eos)\n",
        "    return detokenize(ids, itos)\n",
        "\n",
        "# -------------------------------\n",
        "# 7f) Fusion generator (with class mixture sampling) with prompt\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def generate_fusion(model, stoi, itos, fb, max_len=28,\n",
        "                    temperature=0.8, top_p=0.9, control=None,\n",
        "                    prompt: Optional[str] = None):\n",
        "    model.eval()\n",
        "    groups = _VOCAB_GROUPS\n",
        "    grammar = Grammar()\n",
        "    bos, eos, pad = stoi[\"<bos>\"], stoi[\"<eos>\"], stoi[\"<pad>\"]\n",
        "\n",
        "    ids = [bos]\n",
        "    state = grammar.start_state\n",
        "\n",
        "    # Prompt prefix\n",
        "    if prompt:\n",
        "        prefix_toks = tokenize_prompt(prompt)\n",
        "        state = infer_state_from_prefix(prefix_toks, stoi, itos, groups)\n",
        "        ids += [stoi[t] for t in prefix_toks]\n",
        "\n",
        "    ctrl = control or {}\n",
        "    pos = ctrl.get(\"pos_high\", 0.0) - ctrl.get(\"neg_high\", 0.0)\n",
        "    neg = ctrl.get(\"neg_high\", 0.0) - ctrl.get(\"pos_high\", 0.0)\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        x = torch.tensor(ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "        L = x.size(1)\n",
        "        M = build_semantics_for_prefix(ids, itos, fb, max_len)\n",
        "        if M.size(0) > L: M = M[:L,:]\n",
        "        elif M.size(0) < L:\n",
        "            M = torch.cat([M, torch.zeros(L - M.size(0), M.size(1))], dim=0)\n",
        "        M = M.to(DEVICE).unsqueeze(0)\n",
        "\n",
        "        attn_mask = (x == pad)\n",
        "        logits, sem_pred, _ = model(x, M, attn_mask=attn_mask)\n",
        "        next_logits = logits[0, -1, :]\n",
        "\n",
        "        allow = allowed_ids_for_state(state, groups)\n",
        "\n",
        "        # HARD class restriction under strong control\n",
        "        if state == 7 and allow:\n",
        "            if pos > 0.6:\n",
        "                allow = list(set(allow).intersection(set(groups.pos_adj.tolist())))\n",
        "            elif neg > 0.6:\n",
        "                allow = list(set(allow).intersection(set(groups.neg_adj.tolist())))\n",
        "\n",
        "        # HARD punctuation under strong control\n",
        "        if state == 8 and allow:\n",
        "            if ctrl.get(\"is_question\", 0.0) > 0.6 and groups.qmark in allow:\n",
        "                nid = groups.qmark\n",
        "                ids.append(nid); state = next_state(state)\n",
        "                if state == grammar.end_state: break\n",
        "                continue\n",
        "            if (pos > 0.6) and (ctrl.get(\"str_high\", 0.0) > 0.6) and groups.exclam in allow:\n",
        "                nid = groups.exclam\n",
        "                ids.append(nid); state = next_state(state)\n",
        "                if state == grammar.end_state: break\n",
        "                continue\n",
        "\n",
        "        # Class-mixture sampling at ADJ under strong control\n",
        "        strong = (pos > 0.6) or (neg > 0.6)\n",
        "        if state == 7 and allow and strong:\n",
        "            if pos > 0.6:\n",
        "                nid = sample_group_mixture(next_logits, allow, temperature=1.5, top_p=1.0, alpha_uniform=0.97)\n",
        "            else:\n",
        "                nid = sample_group_mixture(next_logits, allow, temperature=1.3, top_p=0.95, alpha_uniform=0.85)\n",
        "            ids.append(nid)\n",
        "            state = next_state(state)\n",
        "            if state == grammar.end_state: break\n",
        "            continue\n",
        "\n",
        "        # Otherwise: grammar mask + steering + moderate repetition guard\n",
        "        mask = torch.full_like(next_logits, float(\"-inf\"))\n",
        "        if allow:\n",
        "            mask[torch.tensor(allow, device=next_logits.device)] = 0.0\n",
        "        next_logits = next_logits + mask\n",
        "\n",
        "        next_logits = apply_logit_steer(next_logits, ctrl, groups, state)\n",
        "        next_logits = simple_repetition_penalty_k(next_logits, ids, k=3, penalty=1.5, itos=itos)\n",
        "\n",
        "        nid = top_p_top_k_sample(next_logits, temperature, top_p, top_k=0)\n",
        "        ids.append(nid)\n",
        "\n",
        "        state = next_state(state)\n",
        "        if state == grammar.end_state:\n",
        "            break\n",
        "\n",
        "    ids.append(eos)\n",
        "    return detokenize(ids, itos)\n",
        "\n",
        "# -------------------------------\n",
        "# 8) Schedulers\n",
        "# -------------------------------\n",
        "def build_warmup_cosine_scheduler(optimizer, num_warmup, num_steps):\n",
        "    def lr_lambda(step):\n",
        "        if step < num_warmup:\n",
        "            return float(step) / max(1, num_warmup)\n",
        "        progress = float(step - num_warmup) / max(1, num_steps - num_warmup)\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# -------------------------------\n",
        "# 9) Run experiment\n",
        "# -------------------------------\n",
        "def run_experiment():\n",
        "    # Hyperparams\n",
        "    max_len = 28\n",
        "    batch_size = 64\n",
        "    epochs = 6\n",
        "    d_model = 128\n",
        "    nhead = 4\n",
        "    nlayers = 4\n",
        "    dim_ff = 256\n",
        "    aux_lambda = 0.5\n",
        "    base_lr = 3e-4\n",
        "\n",
        "    # OOD adjective split\n",
        "    rng = random.Random(SEED)\n",
        "    pos_all = ADJ_POS[:]; neg_all = ADJ_NEG[:]\n",
        "    rng.shuffle(pos_all); rng.shuffle(neg_all)\n",
        "    split_pos = len(pos_all)//2\n",
        "    split_neg = len(neg_all)//2\n",
        "    ADJ_POS_TRAIN = pos_all[:split_pos]; ADJ_POS_HOLD = pos_all[split_pos:]\n",
        "    ADJ_NEG_TRAIN = neg_all[:split_neg]; ADJ_NEG_HOLD = neg_all[split_neg:]\n",
        "\n",
        "    # Data (train uses only TRAIN subsets; val uses FULL sets)\n",
        "    train_sents, val_sents = make_corpus(\n",
        "        n_train=8000, n_val=1200, max_len=max_len-2,\n",
        "        adj_pos_train=ADJ_POS_TRAIN, adj_neg_train=ADJ_NEG_TRAIN,\n",
        "        adj_pos_val=ADJ_POS,        adj_neg_val=ADJ_NEG\n",
        "    )\n",
        "    stoi, itos = build_vocab(train_sents + val_sents)\n",
        "    fb = build_feature_bank()\n",
        "    feat_dim = len(fb.names)\n",
        "    pad_id = stoi[\"<pad>\"]\n",
        "\n",
        "    # hold-out token ids for seen-only PPL\n",
        "    holdout_ids = set([stoi[w] for w in (ADJ_POS_HOLD + ADJ_NEG_HOLD) if w in stoi])\n",
        "\n",
        "    # Build vocab groups for losses/generation\n",
        "    global _VOCAB_GROUPS\n",
        "    _VOCAB_GROUPS = build_vocab_groups(stoi)\n",
        "\n",
        "    train_ds = LMDataset(train_sents, stoi, fb, max_len=max_len)\n",
        "    val_ds   = LMDataset(val_sents,   stoi, fb, max_len=max_len)\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # -------- Baseline --------\n",
        "    base = TinyTransformerLM(len(stoi), d_model, nhead, nlayers, dim_ff).to(DEVICE)\n",
        "    opt_b = torch.optim.AdamW(base.parameters(), lr=base_lr, weight_decay=0.01)\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    warmup_steps = max(1, int(0.10 * total_steps))\n",
        "    sch_b = build_warmup_cosine_scheduler(opt_b, warmup_steps, total_steps)\n",
        "    global_step_b = 0\n",
        "\n",
        "    def eval_perplexity(model, loader):\n",
        "        model.eval()\n",
        "        total_loss, total_tok = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for x,y,mask,M in loader:\n",
        "                x,y,mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n",
        "                attn_mask = (x==pad_id)\n",
        "                logits,_ = model(x, attn_mask=attn_mask)\n",
        "                loss = lm_step(logits, y, mask, label_smoothing=0.0)  # eval w/o LS\n",
        "                total_loss += loss.item() * mask.sum().item()\n",
        "                total_tok  += mask.sum().item()\n",
        "        ppl = math.exp(total_loss / max(total_tok,1.0))\n",
        "        return ppl\n",
        "\n",
        "    def eval_perplexity_seen_only(model, loader, holdout_ids: set):\n",
        "        model.eval()\n",
        "        total_loss, total_tok = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for x,y,mask,M in loader:\n",
        "                x,y,mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n",
        "                attn_mask = (x==pad_id)\n",
        "                logits,_ = model(x, attn_mask=attn_mask)\n",
        "                vocab = logits.size(-1)\n",
        "                ce = F.cross_entropy(logits.reshape(-1, vocab), y.reshape(-1), reduction=\"none\")\n",
        "                m = mask.reshape(-1)\n",
        "                y_cpu = y.reshape(-1).detach().cpu().numpy()\n",
        "                keep = torch.tensor([int(int(t) not in holdout_ids) for t in y_cpu], device=ce.device, dtype=ce.dtype)\n",
        "                w = m * keep\n",
        "                if w.sum() > 0:\n",
        "                    total_loss += (ce * w).sum().item()\n",
        "                    total_tok  += w.sum().item()\n",
        "        if total_tok == 0:\n",
        "            return float(\"nan\")\n",
        "        return math.exp(total_loss / total_tok)\n",
        "\n",
        "    print(f\"Uniformizer ON? {UNI_ON}  (UNI_LAM={UNI_LAM}, LS_EPS={LS_EPS})\")\n",
        "    print(\"Training baseline...\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        base.train()\n",
        "        for x,y,mask,M in train_loader:\n",
        "            x,y,mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n",
        "            attn_mask = (x==pad_id)\n",
        "            logits,_ = base(x, attn_mask=attn_mask)\n",
        "            # LS + optional uniformizer\n",
        "            loss = lm_step(logits, y, mask, label_smoothing=LS_EPS) \\\n",
        "                   + adj_uniform_kl_loss(logits, y, mask, _VOCAB_GROUPS, lam=UNI_LAM)\n",
        "            opt_b.zero_grad(); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(base.parameters(), 1.0)\n",
        "            opt_b.step(); sch_b.step()\n",
        "            global_step_b += 1\n",
        "        ppl = eval_perplexity(base, val_loader)\n",
        "        print(f\"[Baseline] epoch {ep:02d}  val PPL: {ppl:.3f}\")\n",
        "\n",
        "    base_ppl = eval_perplexity(base, val_loader)\n",
        "    base_seen_ppl = eval_perplexity_seen_only(base, val_loader, holdout_ids)\n",
        "\n",
        "    # -------- Semantic Fusion + Aux --------\n",
        "    fusion = SemanticFusionLM(len(stoi), feat_dim, d_model, nhead, nlayers, dim_ff).to(DEVICE)\n",
        "    opt_f = torch.optim.AdamW(fusion.parameters(), lr=base_lr, weight_decay=0.01)\n",
        "    sch_f = build_warmup_cosine_scheduler(opt_f, warmup_steps, total_steps)\n",
        "    global_step_f = 0\n",
        "\n",
        "    def eval_perplexity_fusion(model, loader):\n",
        "        model.eval()\n",
        "        total_loss, total_tok = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for x,y,mask,M in loader:\n",
        "                x,y,mask,M = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE), M.to(DEVICE)\n",
        "                attn_mask = (x==pad_id)\n",
        "                logits,sem_pred,_ = model(x, M, attn_mask=attn_mask)\n",
        "                loss = lm_step(logits, y, mask, label_smoothing=0.0)\n",
        "                total_loss += loss.item() * mask.sum().item()\n",
        "                total_tok  += mask.sum().item()\n",
        "        ppl = math.exp(total_loss / max(total_tok,1.0))\n",
        "        return ppl\n",
        "\n",
        "    def eval_perplexity_fusion_seen_only(model, loader, holdout_ids: set):\n",
        "        model.eval()\n",
        "        total_loss, total_tok = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for x,y,mask,M in loader:\n",
        "                x,y,mask,M = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE), M.to(DEVICE)\n",
        "                attn_mask = (x==pad_id)\n",
        "                logits,sem_pred,_ = model(x, M, attn_mask=attn_mask)\n",
        "                vocab = logits.size(-1)\n",
        "                ce = F.cross_entropy(logits.reshape(-1, vocab), y.reshape(-1), reduction=\"none\")\n",
        "                m = mask.reshape(-1)\n",
        "                y_cpu = y.reshape(-1).detach().cpu().numpy()\n",
        "                keep = torch.tensor([int(int(t) not in holdout_ids) for t in y_cpu], device=ce.device, dtype=ce.dtype)\n",
        "                w = m * keep\n",
        "                if w.sum() > 0:\n",
        "                    total_loss += (ce * w).sum().item()\n",
        "                    total_tok  += w.sum().item()\n",
        "        if total_tok == 0:\n",
        "            return float(\"nan\")\n",
        "        return math.exp(total_loss / total_tok)\n",
        "\n",
        "    print(\"\\nTraining semantic fusion + auxiliary...\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        fusion.train()\n",
        "        for x,y,mask,M in train_loader:\n",
        "            x,y,mask,M = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE), M.to(DEVICE)\n",
        "            attn_mask = (x==pad_id)\n",
        "            logits,sem_pred,_ = fusion(x, M, attn_mask=attn_mask)\n",
        "            loss_lm = lm_step(logits, y, mask, label_smoothing=LS_EPS)\n",
        "            loss_aux = bce_step(sem_pred, M, mask)\n",
        "            loss_uni = adj_uniform_kl_loss(logits, y, mask, _VOCAB_GROUPS, lam=UNI_LAM)\n",
        "            loss = loss_lm + aux_lambda * loss_aux + loss_uni\n",
        "            opt_f.zero_grad(); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(fusion.parameters(), 1.0)\n",
        "            opt_f.step(); sch_f.step()\n",
        "            global_step_f += 1\n",
        "        ppl = eval_perplexity_fusion(fusion, val_loader)\n",
        "        print(f\"[Fusion+Aux] epoch {ep:02d}  val PPL: {ppl:.3f}\")\n",
        "\n",
        "    fusion_ppl = eval_perplexity_fusion(fusion, val_loader)\n",
        "    fusion_seen_ppl = eval_perplexity_fusion_seen_only(fusion, val_loader, holdout_ids)\n",
        "\n",
        "    def sem_mse(model, loader):\n",
        "        model.eval()\n",
        "        tot, cnt = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for x,y,mask,M in loader:\n",
        "                x,mask,M = x.to(DEVICE), mask.to(DEVICE), M.to(DEVICE)\n",
        "                attn_mask = (x==pad_id)\n",
        "                logits,sem_pred,_ = model(x, M, attn_mask=attn_mask)\n",
        "                err = ((sem_pred - M)**2).mean(dim=-1)\n",
        "                tot += (err * mask).sum().item()\n",
        "                cnt += mask.sum().item()\n",
        "        return tot / max(cnt,1.0)\n",
        "\n",
        "    mse = sem_mse(fusion, val_loader)\n",
        "\n",
        "    print(\"\\n======== RESULTS ========\")\n",
        "    print(f\"Baseline PPL        : {base_ppl:.3f}\")\n",
        "    print(f\"Fusion+Aux PPL      : {fusion_ppl:.3f}\")\n",
        "    print(f\"(Seen-only) Baseline PPL : {base_seen_ppl:.3f}\")\n",
        "    print(f\"(Seen-only) Fusion  PPL  : {fusion_seen_ppl:.3f}\")\n",
        "    print(f\"Semantic pred MSE â†“ : {mse:.4f}  (lower is better)\")\n",
        "    print(\"=========================\")\n",
        "\n",
        "    # Per-token CE (baseline + fusion)\n",
        "    def per_token_loss_base(model, loader, focus: List[str], stoi):\n",
        "        model.eval()\n",
        "        tok_ids = [stoi[t] for t in focus if t in stoi]\n",
        "        total = {t: [0.0, 0.0] for t in tok_ids}\n",
        "        with torch.no_grad():\n",
        "            for x,y,mask,M in loader:\n",
        "                x,y,mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n",
        "                attn_mask = (x == stoi[\"<pad>\"])\n",
        "                logits,_ = model(x, attn_mask=attn_mask)\n",
        "                ce = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1), reduction=\"none\")\n",
        "                ce = ce.reshape(x.size(0), x.size(1))\n",
        "                for tid in tok_ids:\n",
        "                    hit = (y == tid).float()\n",
        "                    total[tid][0] += (ce * hit).sum().item()\n",
        "                    total[tid][1] += hit.sum().item()\n",
        "        return {focus[i]: (total[tok_ids[i]][0] / max(total[tok_ids[i]][1],1.0)) for i in range(len(tok_ids))}\n",
        "\n",
        "    def per_token_loss_fusion(model, loader, focus: List[str], stoi):\n",
        "        model.eval()\n",
        "        tok_ids = [stoi[t] for t in focus if t in stoi]\n",
        "        total = {t: [0.0, 0.0] for t in tok_ids}\n",
        "        with torch.no_grad():\n",
        "            for x,y,mask,M in loader:\n",
        "                x,y,mask,M = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE), M.to(DEVICE)\n",
        "                attn_mask = (x == stoi[\"<pad>\"])\n",
        "                logits,_,_ = model(x, M, attn_mask=attn_mask)\n",
        "                ce = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1), reduction=\"none\")\n",
        "                ce = ce.reshape(x.size(0), x.size(1))\n",
        "                for tid in tok_ids:\n",
        "                    hit = (y == tid).float()\n",
        "                    total[tid][0] += (ce * hit).sum().item()\n",
        "                    total[tid][1] += hit.sum().item()\n",
        "        return {focus[i]: (total[tok_ids[i]][0] / max(total[tok_ids[i]][1],1.0)) for i in range(len(tok_ids))}\n",
        "\n",
        "    focus = [\"good\",\"great\",\"terrible\",\"slightly\",\"very\",\"!\",\"?\",\",\"]\n",
        "    print(\"Baseline focus CE:\", per_token_loss_base(base, val_loader, focus, stoi))\n",
        "    print(\"Fusion   focus CE:\", per_token_loss_fusion(fusion, val_loader, focus, stoi))\n",
        "\n",
        "    # ---- Generations (1-clause for clarity) ----\n",
        "    print(\"\\n--- Baseline generations (FAIR: grammar + last-3 penalty) ---\")\n",
        "    for _ in range(3):\n",
        "        print(\"â€¢\", generate_baseline_fair(base, stoi, itos, max_len=max_len, temperature=0.7, top_p=0.9, top_k=20))\n",
        "\n",
        "    print(\"\\n--- Fusion (neutral) generations ---\")\n",
        "    for _ in range(3):\n",
        "        print(\"â€¢\", generate_fusion(fusion, stoi, itos, fb, max_len=max_len, temperature=0.7, top_p=0.9, control=None))\n",
        "\n",
        "    print(\"\\n--- Fusion (controlled: positive & strong) ---\")\n",
        "    pos_strong = {\"pos_high\": 0.95, \"str_high\": 0.9}\n",
        "    for _ in range(3):\n",
        "        print(\"â€¢\", generate_fusion(fusion, stoi, itos, fb, max_len=max_len,\n",
        "                                   temperature=0.7, top_p=0.9, control=pos_strong))\n",
        "\n",
        "    print(\"\\n--- Fusion (controlled: negative & question) ---\")\n",
        "    neg_question = {\"neg_high\": 0.95, \"is_question\": 1.0, \"str_med\": 0.6}\n",
        "    for _ in range(3):\n",
        "        print(\"â€¢\", generate_fusion(fusion, stoi, itos, fb, max_len=max_len,\n",
        "                                   temperature=0.8, top_p=0.9, control=neg_question))\n",
        "\n",
        "    # ---- Prompted ----\n",
        "    print(\"\\n--- Prompted ---\")\n",
        "    prompt = \"Carol starts the model,\"  # must follow the grammar: SUBJ VERB 'the' OBJ ','\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Baseline (fair):\", generate_baseline_fair(\n",
        "        base, stoi, itos, max_len=max_len, temperature=0.7, top_p=0.9, top_k=20,\n",
        "        prompt=prompt))\n",
        "    print(\"Fusion (no control)        :\", generate_fusion(\n",
        "        fusion, stoi, itos, fb, max_len=max_len, temperature=0.7, top_p=0.9,\n",
        "        control=None, prompt=prompt))\n",
        "    print(\"Fusion (controlled: positive & strong)        :\", generate_fusion(\n",
        "        fusion, stoi, itos, fb, max_len=max_len, temperature=0.7, top_p=0.9,\n",
        "        control=pos_strong, prompt=prompt))\n",
        "    print(\"Fusion (controlled: negative & question)        :\", generate_fusion(\n",
        "        fusion, stoi, itos, fb, max_len=max_len, temperature=0.7, top_p=0.9,\n",
        "        control=neg_question, prompt=prompt))\n",
        "\n",
        "    # ---- Control success sanity check ----\n",
        "    def last_punct(s: str):\n",
        "        return s.strip()[-1] if s.strip() and s.strip()[-1] in \".!?\" else \"\"\n",
        "    def last_adj(s: str):\n",
        "        m = re.search(r\",\\s+(?:[a-z]+\\s+)?([a-z]+)[.!?]$\", s.strip())\n",
        "        return m.group(1) if m else \"\"\n",
        "\n",
        "    def control_success(fusion, stoi, itos, fb, N=200):\n",
        "        pos_ctrl = {\"pos_high\":0.95, \"str_high\":0.9}\n",
        "        neg_ctrl = {\"neg_high\":0.95, \"is_question\":1.0, \"str_med\":0.6}\n",
        "        counts = {\"pos_adj\":0, \"pos_exc\":0, \"neg_adj\":0, \"neg_q\":0}\n",
        "        pos_set = {w.lower() for w in ADJ_POS}\n",
        "        neg_set = {w.lower() for w in ADJ_NEG}\n",
        "        for _ in range(N):\n",
        "            s1 = generate_fusion(fusion, stoi, itos, fb, control=pos_ctrl, temperature=0.7, top_p=0.9)\n",
        "            a1, p1 = last_adj(s1), last_punct(s1)\n",
        "            counts[\"pos_adj\"] += int(a1 in pos_set)\n",
        "            counts[\"pos_exc\"] += int(p1 == \"!\")\n",
        "            s2 = generate_fusion(fusion, stoi, itos, fb, control=neg_ctrl, temperature=0.8, top_p=0.9)\n",
        "            a2, p2 = last_adj(s2), last_punct(s2)\n",
        "            counts[\"neg_adj\"] += int(a2 in neg_set)\n",
        "            counts[\"neg_q\"]   += int(p2 == \"?\")\n",
        "        for k in counts: counts[k] /= N\n",
        "        print(\"\\nControl success rates over\", N, \"samples:\")\n",
        "        print(\"  positive: adj=\", counts[\"pos_adj\"], \"  !=\", counts[\"pos_exc\"])\n",
        "        print(\"  negative: adj=\", counts[\"neg_adj\"], \"  ?=\", counts[\"neg_q\"])\n",
        "\n",
        "    control_success(fusion, stoi, itos, fb, N=200)\n",
        "\n",
        "    # ---- Confusion table ----\n",
        "    def control_confusion(fusion, stoi, itos, fb, N=200):\n",
        "        pos_ctrl = {\"pos_high\":0.95, \"str_high\":0.9}\n",
        "        neg_ctrl = {\"neg_high\":0.95, \"str_med\":0.6}\n",
        "        pos_set = {w.lower() for w in ADJ_POS}\n",
        "        neg_set = {w.lower() for w in ADJ_NEG}\n",
        "        mat = np.zeros((2,3), dtype=int)\n",
        "        def cls(word):\n",
        "            if word in pos_set: return 0\n",
        "            if word in neg_set: return 1\n",
        "            return 2\n",
        "        for _ in range(N):\n",
        "            s1 = generate_fusion(fusion, stoi, itos, fb, control=pos_ctrl, temperature=0.7, top_p=0.9)\n",
        "            mat[0, cls(last_adj(s1))] += 1\n",
        "            s2 = generate_fusion(fusion, stoi, itos, fb, control=neg_ctrl, temperature=0.8, top_p=0.9)\n",
        "            mat[1, cls(last_adj(s2))] += 1\n",
        "        def rowfmt(r):\n",
        "            tot = mat[r].sum()\n",
        "            return f\"{mat[r,0]:4d} ({mat[r,0]/max(tot,1):.2f}) | {mat[r,1]:4d} ({mat[r,1]/max(tot,1):.2f}) | {mat[r,2]:4d} ({mat[r,2]/max(tot,1):.2f})\"\n",
        "        print(\"\\nConfusion table (rows=intended, cols=realized):\")\n",
        "        print(\"               POS         NEG        OTHER\")\n",
        "        print(f\"Intended POS : {rowfmt(0)}\")\n",
        "        print(f\"Intended NEG : {rowfmt(1)}\")\n",
        "\n",
        "    control_confusion(fusion, stoi, itos, fb, N=200)\n",
        "\n",
        "    # ---- OOD test (held-out adjectives) ----\n",
        "    def ood_test(fusion, stoi, itos, fb, adj_pos_hold, adj_neg_hold, N=200):\n",
        "        pos_ctrl = {\"pos_high\":0.95, \"str_high\":0.9}\n",
        "        neg_ctrl = {\"neg_high\":0.95, \"str_med\":0.6}\n",
        "        pos_hold = {w.lower() for w in adj_pos_hold}\n",
        "        neg_hold = {w.lower() for w in adj_neg_hold}\n",
        "        pos_hit = 0; pos_tot = 0\n",
        "        neg_hit = 0; neg_tot = 0\n",
        "        for _ in range(N):\n",
        "            s1 = generate_fusion(fusion, stoi, itos, fb, control=pos_ctrl, temperature=0.7, top_p=0.9)\n",
        "            pos_hit += int(last_adj(s1) in pos_hold); pos_tot += 1\n",
        "            s2 = generate_fusion(fusion, stoi, itos, fb, control=neg_ctrl, temperature=0.8, top_p=0.9)\n",
        "            neg_hit += int(last_adj(s2) in neg_hold); neg_tot += 1\n",
        "        print(\"\\nOOD control test (held-out adjectives):\")\n",
        "        print(f\"  POS control -> holdout adj rate: {pos_hit}/{pos_tot} = {pos_hit/max(pos_tot,1):.2f}\")\n",
        "        print(f\"  NEG control -> holdout adj rate: {neg_hit}/{neg_tot} = {neg_hit/max(neg_tot,1):.2f}\")\n",
        "        print(\"  (Higher is better; shows the controller can target words unseen in training.)\")\n",
        "\n",
        "    print(\"\\nHeld-out adjectives (POS):\", ADJ_POS_HOLD)\n",
        "    print(\"Held-out adjectives (NEG):\", ADJ_NEG_HOLD)\n",
        "    ood_test(fusion, stoi, itos, fb, ADJ_POS_HOLD, ADJ_NEG_HOLD, N=200)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxnRnwRpGzp6",
        "outputId": "045afd08-e47f-4a49-f597-43f25a32f3e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniformizer ON? True  (UNI_LAM=0.01, LS_EPS=0.02)\n",
            "Training baseline...\n",
            "[Baseline] epoch 01  val PPL: 8.346\n",
            "[Baseline] epoch 02  val PPL: 3.341\n",
            "[Baseline] epoch 03  val PPL: 2.470\n",
            "[Baseline] epoch 04  val PPL: 2.316\n",
            "[Baseline] epoch 05  val PPL: 2.257\n",
            "[Baseline] epoch 06  val PPL: 2.249\n",
            "\n",
            "Training semantic fusion + auxiliary...\n",
            "[Fusion+Aux] epoch 01  val PPL: 5.474\n",
            "[Fusion+Aux] epoch 02  val PPL: 2.741\n",
            "[Fusion+Aux] epoch 03  val PPL: 2.208\n",
            "[Fusion+Aux] epoch 04  val PPL: 2.213\n",
            "[Fusion+Aux] epoch 05  val PPL: 2.160\n",
            "[Fusion+Aux] epoch 06  val PPL: 2.152\n",
            "\n",
            "======== RESULTS ========\n",
            "Baseline PPL        : 2.249\n",
            "Fusion+Aux PPL      : 2.152\n",
            "(Seen-only) Baseline PPL : 1.511\n",
            "(Seen-only) Fusion  PPL  : 1.431\n",
            "Semantic pred MSE â†“ : 0.0087  (lower is better)\n",
            "=========================\n",
            "Baseline focus CE: {'good': 0.0025832652481216373, 'great': 6.86867622159562, 'terrible': 7.021536509195964, 'slightly': 0.0030900408392367158, 'very': 0.0027665011992212387, '!': 4.4212874131237, '?': 3.9464013726861626, ',': 0.0029254426892907507}\n",
            "Fusion   focus CE: {'good': 0.0017719236766975937, 'great': 8.948555424528301, 'terrible': 6.987641547912876, 'slightly': 0.0025366929101862134, 'very': 0.0019137949275318533, '!': 2.8817217984645485, '?': 2.9145647384024955, ',': 0.003542499891783273}\n",
            "\n",
            "--- Baseline generations (FAIR: grammar + last-3 penalty) ---\n",
            "â€¢ Dave starts the paper, moderately good.\n",
            "â€¢ Dave starts the paper, moderately good.\n",
            "â€¢ Dave starts the paper, moderately good.\n",
            "\n",
            "--- Fusion (neutral) generations ---\n",
            "â€¢ Alice cooks the model, moderately pleasant!\n",
            "â€¢ Alice cooks the model, moderately pleasant!\n",
            "â€¢ Alice reviews the model, moderately pleasant!\n",
            "\n",
            "--- Fusion (controlled: positive & strong) ---\n",
            "â€¢ Alice reviews the paper, moderately excellent!\n",
            "â€¢ Carol finishes the model, extremely wonderful!\n",
            "â€¢ Alice reviews the model, moderately pleasant!\n",
            "\n",
            "--- Fusion (controlled: negative & question) ---\n",
            "â€¢ Carol finishes the model, moderately bad?\n",
            "â€¢ Alice cooks the model, moderately terrible?\n",
            "â€¢ Alice reviews the model, extremely awful?\n",
            "\n",
            "--- Prompted ---\n",
            "Prompt: Carol starts the model,\n",
            "Baseline (fair): Carol starts the model, slightly good.\n",
            "Fusion (no control)        : Carol starts the model, extremely pleasant!\n",
            "Fusion (controlled: positive & strong)        : Carol starts the model, extremely excellent!\n",
            "Fusion (controlled: negative & question)        : Carol starts the model, extremely bad?\n",
            "\n",
            "Control success rates over 200 samples:\n",
            "  positive: adj= 1.0   != 1.0\n",
            "  negative: adj= 1.0   ?= 1.0\n",
            "\n",
            "Confusion table (rows=intended, cols=realized):\n",
            "               POS         NEG        OTHER\n",
            "Intended POS :  200 (1.00) |    0 (0.00) |    0 (0.00)\n",
            "Intended NEG :    0 (0.00) |  200 (1.00) |    0 (0.00)\n",
            "\n",
            "Held-out adjectives (POS): ['wonderful', 'excellent', 'great']\n",
            "Held-out adjectives (NEG): ['terrible', 'awful', 'unpleasant']\n",
            "\n",
            "OOD control test (held-out adjectives):\n",
            "  POS control -> holdout adj rate: 124/200 = 0.62\n",
            "  NEG control -> holdout adj rate: 87/200 = 0.43\n",
            "  (Higher is better; shows the controller can target words unseen in training.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# end."
      ],
      "metadata": {
        "id": "zgCq32GptuRP"
      }
    }
  ]
}